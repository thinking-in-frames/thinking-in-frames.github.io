<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning - Chengzu Li, Zanyi Wang, Jiaang Li, Yi Xu, Han Zhou, Huanyu Zhang, Ruichuan An, Dengyang Jiang, Zhaochong An, Ivan Vuliƒá, Serge Belongie, Anna Korhonen">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="By thinking in frames, we show that visual context and test-time scaling significantly improve video reasoning capabilities in Out-of-Distribution scenarios.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <!-- <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI"> -->
  <!-- TODO: List all authors -->
  <meta name="author" content="Chengzu Li, Zanyi Wang, Jiaang Li, Yi Xu, Han Zhou, Huanyu Zhang, Ruichuan An, Dengyang Jiang, Zhaochong An, Ivan Vuliƒá, Serge Belongie, Anna Korhonen">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="University of Cambridge">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="By thinking in frames, we show that visual context and test-time scaling significantly improve video reasoning capabilities in Out-of-Distribution scenarios.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://thinking-in-frames.github.io/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://thinking-in-frames.github.io/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Chengzu Li">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Visual Reasoning">
  <meta property="article:tag" content="Test-Time Scaling">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@CambridgeLTL">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@li_chengzu">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="By thinking in frames, we show that visual context and test-time scaling significantly improve video reasoning capabilities in Out-of-Distribution scenarios.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://thinking-in-frames.github.io/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning">
  <meta name="citation_author" content="Li, Chengzu">
  <meta name="citation_author" content="Wang, Zanyi">
  <meta name="citation_author" content="Li, Jiaang">
  <meta name="citation_author" content="Xu, Yi">
  <meta name="citation_author" content="Zhou, Han">
  <meta name="citation_author" content="Zhang, Huanyu">
  <meta name="citation_author" content="An, Ruichuan">
  <meta name="citation_author" content="Jiang, Dengyang">
  <meta name="citation_author" content="An, Zhaochong">
  <meta name="citation_author" content="Vuliƒá, Ivan">
  <meta name="citation_author" content="Belongie, Serge">
  <meta name="citation_author" content="Korhonen, Anna">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="arXiv">
  <meta name="citation_pdf_url" content="https://www.arxiv.org/pdf/2601.21037">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning - Chengzu Li, Zanyi Wang, Jiaang Li, Yi Xu, Han Zhou, Huanyu Zhang, Ruichuan An, Dengyang Jiang, Zhaochong An, Ivan Vuliƒá, Serge Belongie, Anna Korhonen | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  <style>
    /* Leaderboard Styles */
    .leaderboard-section {
      padding: 30px 20px 10px;
      background: #ffffff;
    }

    .leaderboard-card {
      background: #fff;
      border-radius: 12px;
      box-shadow: 0 10px 25px rgba(0, 0, 0, 0.08);
      padding: 20px 18px;
    }

    .leaderboard-title {
      text-align: center;
      font-size: 1.2rem;
      margin-bottom: 16px;
      color: #222;
    }

    .leaderboard-subtitle {
      text-align: left;
      color: #666;
      font-size: 0.9rem;
      margin-top: -8px;
      margin-bottom: 18px;
    }

    .leaderboard-table-wrap {
      overflow-x: auto;
    }

    .leaderboard-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.9rem;
      min-width: 980px;
    }

    .leaderboard-table th,
    .leaderboard-table td {
      padding: 10px 8px;
      border-bottom: 1px solid #e9ecef;
      text-align: center;
      white-space: nowrap;
    }

    .leaderboard-table th:first-child,
    .leaderboard-table td:first-child {
      text-align: left;
    }

    .leaderboard-table th:not(:first-child),
    .leaderboard-table td:not(:first-child) {
      text-align: center !important;
    }

    .leaderboard-table .group-divider {
      border-left: 1px solid #eef1f4;
    }

    /* Maze table: body vertical dividers between groups (columns are 1..17) */
    .maze-table tbody td:nth-child(6),
    .maze-table tbody td:nth-child(10),
    .maze-table tbody td:nth-child(14) {
      border-left: 1px solid #eef1f4;
    }

    /* Maze table header dividers (last header row contains metric-level ths at cols 4..17) */
    .maze-table thead tr:last-child th:nth-child(3),
    .maze-table thead tr:last-child th:nth-child(7),
    .maze-table thead tr:last-child th:nth-child(11),
    .maze-table thead tr:last-child th:nth-child(15) {
      border-left: 1px solid #eef1f4;
      padding-left: 10px;
    }

    /* Tangram table: single divider between Seen (cols 4-6) and Unseen (cols 7-9) */
    .tangram-table thead tr:last-child th:nth-child(7),
    .tangram-table tbody td:nth-child(7) {
      border-left: 1px solid #eef1f4;
      padding-left: 10px;
    }

    .leaderboard-table thead th {
      background: #f7f8fa;
      color: #333;
      font-weight: 600;
    }

    /* Ensure leftmost header cells with rowspans vertically center across rows */
    .leaderboard-table thead th[rowspan] {
      vertical-align: middle;
      text-align: left;
      padding-left: 12px;
    }

    .leaderboard-table .model-cell {
      text-align: left;
      font-weight: 600;
      color: #222;
    }

    .leaderboard-table .group-header {
      background: #f1f3f6;
      font-weight: 700;
    }

    .leaderboard-table .best {
      background: #e6f9ff; /* light cyan */
      color: inherit;
      /* font-weight: 600; */
    }

    .leaderboard-table .second {
      color: inherit;
      font-weight: inherit;
    }

    .leaderboard-table .divider-row td {
      border-top: 2px solid #dfe3e8;
    }

    .leaderboard-table .overall {
      background: #e6f9ff; /* light cyan */
      color: inherit;
      /* font-weight: 600; */
    }
    /* Video Demo Toggle & Grid Styles */
    .video-demos-toggle {
      display: inline-block;
      margin-top: 12px;
      padding: 8px 16px;
      background: #2563eb;
      color: white;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      font-weight: 600;
      font-size: 0.95rem;
      transition: background 0.2s ease;
    }

    .video-demos-toggle:hover {
      background: #1d4ed8;
    }

    .video-demos-toggle.active {
      background: #1d4ed8;
    }

    .video-demos-container {
      max-height: 0;
      overflow: hidden;
      opacity: 0;
      transition: max-height 0.3s ease, opacity 0.3s ease;
      margin-top: 12px;
    }

    .video-demos-container.active {
      max-height: 2000px;
      opacity: 1;
    }

    .video-demos-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 16px;
      margin-top: 16px;
    }

    /* Tangram-specific grid: columns for Fade:Rotation:Translation = 1:2:2 */
    .tangram-demos-grid {
      display: grid;
      grid-template-columns: 1fr 2fr 2fr;
      gap: 16px;
      margin-top: 16px;
    }

    @media (max-width: 768px) {
      .video-demos-grid {
        grid-template-columns: repeat(2, 1fr);
      }
      .tangram-demos-grid {
        grid-template-columns: repeat(2, 1fr);
      }
    }

    @media (max-width: 480px) {
      .video-demos-grid {
        grid-template-columns: 1fr;
      }
      .tangram-demos-grid {
        grid-template-columns: 1fr;
      }
    }

    .video-demo-item {
      display: flex;
      flex-direction: column;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      transition: box-shadow 0.2s ease;
    }

    .video-demo-item:hover {
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
    }

    .video-demo-item video {
      width: 100%;
      height: auto;
      display: block;
    }

    .video-demo-label {
      padding: 8px 12px;
      background: #f7f8fa;
      font-size: 0.85rem;
      font-weight: 600;
      color: #333;
      text-align: center;
    }

    /* Tangram video sizing - maintain aspect ratio, use rotation as reference height */
    .tangram-demo-item.fade video {
      max-height: 360px;
      width: auto;
    }

    .tangram-demo-item.rotation video,
    .tangram-demo-item.translation video {
      max-height: 360px;
      width: auto;
    }
  </style>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <script>
    function toggleVideoDemos() {
      const container = document.getElementById('videoDemosContainer');
      const buttons = document.querySelectorAll('.video-demos-toggle');
      const button = buttons[0];
      
      if (container.classList.contains('active')) {
        container.classList.remove('active');
        button.classList.remove('active');
        button.textContent = 'üìΩÔ∏è Show Maze Demo Videos';
      } else {
        container.classList.add('active');
        button.classList.add('active');
        button.textContent = 'üìΩÔ∏è Hide Maze Demo Videos';
      }
    }

    function toggleTangramDemos() {
      const container = document.getElementById('tangramDemosContainer');
      const buttons = document.querySelectorAll('.video-demos-toggle');
      const button = buttons[1];
      
      if (container.classList.contains('active')) {
        container.classList.remove('active');
        button.classList.remove('active');
        button.textContent = 'üìΩÔ∏è Show Tangram Demo Videos';
      } else {
        container.classList.add('active');
        button.classList.add('active');
        button.textContent = 'üìΩÔ∏è Hide Tangram Demo Videos';
      }
    }

    function initializeVideoPoster() {
      const videos = document.querySelectorAll('.video-demo-item video');
      videos.forEach(video => {
        video.addEventListener('loadedmetadata', function() {
          // Seek to first frame to capture thumbnail
          this.currentTime = 0.1;
        });
        
        video.addEventListener('seeked', function() {
          if (!this.dataset.posterSet) {
            // Capture current frame and set as poster
            const canvas = document.createElement('canvas');
            canvas.width = this.videoWidth;
            canvas.height = this.videoHeight;
            const ctx = canvas.getContext('2d');
            ctx.drawImage(this, 0, 0);
            
            const posterDataUrl = canvas.toDataURL('image/jpeg');
            this.poster = posterDataUrl;
            this.currentTime = 0; // Reset to beginning
            this.dataset.posterSet = 'true';
          }
        }, { once: true });
      });
    }

    document.addEventListener('DOMContentLoaded', initializeVideoPoster);
  </script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="https://openreview.net/forum?id=6vk6Xg24ZC" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>Imagine While Reasoning in Space: Multimodal Visualization-of-Thought</h5>
            <!-- TODO: Replace with brief description -->
            <p>Native Multimodal Thinking with Unified Models. </p>
            <!-- TODO: Replace with venue and year -->
            <span class="work-venue">ICML 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <a href="https://openreview.net/forum?id=wsnse46kRO" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Visual Planning: Let's Think Only with Images</h5>
            <p>Pure Visual Planning without Language Using Auto-Regressive Visual Generation Model. </p>
            <span class="work-venue">ICLR 2026</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/2508.20068" class="work-item" target="_blank">
          <div class="work-info">
            <h5>11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis</h5>
            <p>Spatial Cognition Benchmark with Predicability for Multimodal Reasoning.</p>
            <span class="work-venue">arXiv</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-2 publication-title">Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning</h1>
            <div class="is-size-6 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://chengzu-li.github.io/" target="_blank">Chengzu Li</a><sup>*12‚Ä†</sup>,
              </span>
                <span class="author-block">
                    Zanyi Wang<sup>*3</sup>,
                </span>
                <span class="author-block">
                    <a href="https://jiaangli.github.io/" target="_blank">Jiaang Li</a><sup>*2</sup>,
                </span>
                <span class="author-block">
                    <a href="https://yix8.github.io/" target="_blank">Yi Xu</a><sup>1</sup>,
                </span>
                <span class="author-block">
                    <a href="https://hzhou.top/" target="_blank">Han Zhou</a><sup>1</sup>,
                </span>
                <span class="author-block">
                    <a href="https://hwanyu112.github.io/" target="_blank">Huanyu Zhang</a><sup>4</sup>,
                </span>
                <span class="author-block">
                    <a href="https://arctanxarc.github.io/" target="_blank">Ruichuan An</a><sup>5</sup>,
                </span>
                <span class="author-block">
                    <a href="https://vvvvvjdy.github.io/" target="_blank">Dengyang Jiang</a><sup>6</sup>,
                </span>
                <span class="author-block">
                    <a href="https://zhaochongan.github.io/" target="_blank">Zhaochong An</a><sup>2</sup>,
                </span>
                <span class="author-block">
                    <a href="https://sites.google.com/site/ivanvulic/" target="_blank">Ivan Vuliƒá</a><sup>1</sup>,
                </span>
                <span class="author-block">
                    <a href="https://serge.belongie.com/" target="_blank">Serge Belongie</a><sup>2</sup>,
                </span>
                <span class="author-block">
                    <a href="https://sites.google.com/site/annakorhonen/" target="_blank">Anna Korhonen</a><sup>1</sup>
                </span>
                </div>

                <div class="is-size-6 publication-authors" >
                    <span class="author-block">
                      <sup>1</sup>University of Cambridge
                      <sup>2</sup>Pioneer Center for AI, University of Copenhagen
                      <sup>3</sup>University of California San Diego
                      <sup>4</sup>Institute of Automation, CAS
                      <sup>5</sup>Peking University 
                      <sup>6</sup>Hong Kong University of Science and Technology 
                      <!-- Conferance name and year -->
                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution <sup>‚Ä†</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2601.21037.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2601.21037" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
Vision-Language Models have excelled at textual reasoning, but they often struggle with finegrained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps
between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: 
(1) Robust <b>Zero-Shot Generalization</b>: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. 
(2) <b>Visual Context</b>: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain
high visual consistency and adapt its planning capability robustly to unseen patterns. 
(3) <b>Visual Test-Time Scaling</b>: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially
and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <div style="max-width:100%; margin:0 auto;">
        <div class="columns is-variable is-vcentered" style="margin-left:0; margin-right:0;">
          <div class="column">
            <img src="static/images/main_fig.jpg" alt="Teaser Overview Image" style="width:100%; height:auto; border-radius:8px;">
          </div>
        </div>
        <h2 class="subtitle has-text-left" style="font-size:18px; line-height:1.5; font-family:'Noto Sans', sans-serif; width:100%; max-width:100%; margin:8px 0 0;">
        Video generation models as visual reasoners, empowered by 
        (1) enriched visual context for improved geometric control and 
        (2) visual test-time scaling that allocates a larger inference-frame budget and enables stronger performance 
        on long-horizon, complex sequential planning tasks, together demonstrating robust generalization across diverse scenarios.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <h2 class="title is-4 has-text-centered">
        Visual Test-Time Scaling
      </h2>
      <div style="max-width:100%; margin:0 auto;">
        <h2 class="subtitle is-size-5 has-text-centered" style="font-size:18px; line-height:1.5; font-family:'Noto Sans', sans-serif; width:100%; max-width:100%; margin-bottom:20px; font-weight: 600;">
          (1) Number of Inference Frames vs. Task Performance
        </h2>
        <div class="columns is-centered is-variable is-vcentered" style="margin-left:0; margin-right:0;">
          <div class="column is-three-quarters has-text-centered">
            <img src="static/images/visual_test_time_scaling_total_frames.png" alt="Test Time Scaling - Total Frames" style="width:100%; height:auto; border-radius:8px;">
          </div>
        </div>
        <h2 class="subtitle has-text-left" style="font-size:18px; line-height:1.5; font-family:'Noto Sans', sans-serif; width:100%; max-width:100%; margin:-10px 0 20px 0;">
        We find that increasing the total frame count (e.g., from 81 to 101, 121 frames) improves navigation performance on both spatially (maze size) and temporally (path length) OOD tasks.
        OOD performance increases steadily as we scale the inference budget from 61 to 121 frames.
        However, we observe a ceiling effect: when scaling to 141 frames for temporal OOD cases, performance drops compared to 121 frames, though it remains superior to the training baseline of 81 frames. 
        We attribute this drop to the architectural limits of the video generation model's positional embeddings, which struggle to extrapolate when the frame count deviates significantly from the training distribution.
        </h2>
      </div>
      <div style="max-width:100%; margin:0 auto;">
        <h2 class="subtitle is-size-5 has-text-centered" style="font-size:18px; line-height:1.5; font-family:'Noto Sans', sans-serif; width:100%; max-width:100%; margin-bottom:20px; font-weight: 600;">
          (2) Scaling Factors per Steps vs. Task Performance
        </h2>
        <div class="columns is-centered is-variable is-vcentered" style="margin-left:0; margin-right:0;">
          <div class="column is-three-quarters has-text-centered">
            <img src="static/images/visual_test_time_scaling_scaling_factors.png" alt="Test Time Scaling - Scaling Factors" style="width:100%; height:auto; border-radius:8px;">
          </div>
        </div>
        <p class="subtitle has-text-left" style="font-size:18px; line-height:1.5; font-family:'Noto Sans', sans-serif; width:100%; max-width:100%; margin:8px 0;">
        To rigorously probe whether these gains stem from finer-grained reasoning or simply longer video duration, we introduce a control variable Œ∫ (scaling factor), defined as the number of frames allocated per discrete step in the maze solution. 
We tested Œ∫ ‚àà {5, 7, 9, 11} across spatially and temporally OOD settings.
As shown in the figure above, we observe a clear positive correlation: assigning more frames per step (Œ∫=7, 9, 11) significantly improves performance on spatially ID and OOD settings compared to lower resolutions (Œ∫=5). 
Notably, in temporal OOD settings, performance peaks at Œ∫=9 before degrading at Œ∫=11. 
This degradation aligns with the positional embedding limitation noted above, as Œ∫=11 pushes the total video length to ~200 frames. 
Crucially, this drop is <em>not</em> observed in the spatially OOD setting where total path length is in-distribution, confirming that the degradation is limited by the sequence length capacity rather than logical.
</p>
      </div>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-4 has-text-centered">
        Qualitative Examples of Video Reasoning
      </h2>
      <div style="text-align: center;">
        <button class="video-demos-toggle" onclick="toggleVideoDemos()" title="Toggle Maze Demo Videos">
          <i class="fas fa-film" style="margin-right: 6px;"></i>
          Show Maze Demo Videos
        </button>
      </div>
      <div class="video-demos-container" id="videoDemosContainer">
        <div class="video-demos-grid">
          <div class="video-demo-item">
            <video controls muted preload="metadata">
              <source src="static/videos/maze/maze4_1023_00_81inference.mp4" type="video/mp4">
            </video>
            <div class="video-demo-label">Maze 4x4</div>
          </div>
          <div class="video-demo-item">
            <video controls muted preload="metadata">
              <source src="static/videos/maze/maze5_0031_00_81inference.mp4" type="video/mp4">
            </video>
            <div class="video-demo-label">Maze 5x5</div>
          </div>
          <div class="video-demo-item">
            <video controls muted preload="metadata">
              <source src="static/videos/maze/maze6_1023_00_121inference.mp4" type="video/mp4">
            </video>
            <div class="video-demo-label">Maze 6x6</div>
          </div>
          <div class="video-demo-item">
            <video controls muted preload="metadata">
              <source src="static/videos/maze/maze6_1033_00_81inference.mp4" type="video/mp4">
            </video>
            <div class="video-demo-label">Maze 6x6 (Variant)</div>
          </div>
          <div class="video-demo-item">
            <video controls muted preload="metadata">
              <source src="static/videos/maze/maze6ood_0025_00_81inference.mp4" type="video/mp4">
            </video>
            <div class="video-demo-label">Maze 6x6 OOD</div>
          </div>
          <div class="video-demo-item">
            <video controls muted preload="metadata">
              <source src="static/videos/maze/maze7ood_0013_00_81inference.mp4" type="video/mp4">
            </video>
            <div class="video-demo-label">Maze 7x7 OOD</div>
          </div>
          <div class="video-demo-item">
            <video controls muted preload="metadata">
              <source src="static/videos/maze/maze7ood_0021_00_81inference.mp4" type="video/mp4">
            </video>
            <div class="video-demo-label">Maze 7x7 OOD (Variant)</div>
          </div>
          <div class="video-demo-item">
            <video controls muted preload="metadata">
              <source src="static/videos/maze/maze8ood_0025_00_81inference.mp4" type="video/mp4">
            </video>
            <div class="video-demo-label">Maze 8x8 OOD</div>
          </div>
        </div>
      </div>
      <div style="text-align: center;">
        <button class="video-demos-toggle" onclick="toggleTangramDemos()" title="Toggle Tangram Demo Videos">
          <i class="fas fa-film" style="margin-right: 6px;"></i>
          Show Tangram Demo Videos
        </button>
      </div>
      <div class="video-demos-container" id="tangramDemosContainer">
        <div class="video-demos-grid tangram-demos-grid">
          <div class="tangram-column tangram-fade">
            <div class="video-demo-item tangram-demo-item fade">
              <video controls muted preload="metadata">
                <source src="static/videos/tangram/page1-142_00_inference_fade.mp4" type="video/mp4">
              </video>
              <div class="video-demo-label">Fade (142)</div>
            </div>
            <div class="video-demo-item tangram-demo-item fade">
              <video controls muted preload="metadata">
                <source src="static/videos/tangram/page1-172_00_inference_fade.mp4" type="video/mp4">
              </video>
              <div class="video-demo-label">Fade (172)</div>
            </div>
          </div>

          <div class="tangram-column tangram-rotation">
            <div class="video-demo-item tangram-demo-item rotation">
              <video controls muted preload="metadata">
                <source src="static/videos/tangram/page1-142_00_inference_rotation.mp4" type="video/mp4">
              </video>
              <div class="video-demo-label">Rotation (142)</div>
            </div>
            <div class="video-demo-item tangram-demo-item rotation">
              <video controls muted preload="metadata">
                <source src="static/videos/tangram/page1-172_00_inference_rotation.mp4" type="video/mp4">
              </video>
              <div class="video-demo-label">Rotation (172)</div>
            </div>
          </div>

          <div class="tangram-column tangram-translation">
            <div class="video-demo-item tangram-demo-item translation">
              <video controls muted preload="metadata">
                <source src="static/videos/tangram/page1-142_00_inference_translation.mp4" type="video/mp4">
              </video>
              <div class="video-demo-label">Translation (142)</div>
            </div>
            <div class="video-demo-item tangram-demo-item translation">
              <video controls muted preload="metadata">
                <source src="static/videos/tangram/page1-172_00_inference_translation.mp4" type="video/mp4">
              </video>
              <div class="video-demo-label">Translation (172)</div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Result Table -->
<section class="section leaderboard-section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 has-text-centered">Quantitative Results</h2>
    <div class="leaderboard-card">
      <h2 class="leaderboard-title">Maze Navigation Quantitative Results</h2>
      <p class="leaderboard-subtitle">EM = Exact Match, PR = Progress Rate. Proprietary models evaluated zero-shot; open-sourced models fine-tuned. Cyan represents visual reasoning system.</p>
      <div class="leaderboard-table-wrap">
        <table class="leaderboard-table maze-table">
          <thead>
            <tr>
              <th rowspan="3">Model</th>
              <th rowspan="3">Input</th>
              <th rowspan="3">Output</th>
              <th colspan="2">In Distribution</th>
              <th colspan="4">OOD Maze Sizes</th>
              <th colspan="4">OOD Path Length</th>
              <th colspan="4">OOD Both</th>
            </tr>
            <tr>
              <th colspan="2">3x3 -- 6x6</th>
              <th colspan="2">7x7</th>
              <th colspan="2">8x8</th>
              <th colspan="2">5x5 (Long)</th>
              <th colspan="2">6x6 (Long)</th>
              <th colspan="2">7x7 (Long)</th>
              <th colspan="2">8x8 (Long)</th>
            </tr>
            <tr>
              <th>EM</th><th>PR</th>
              <th>EM</th><th>PR</th><th>EM</th><th>PR</th>
              <th>EM</th><th>PR</th><th>EM</th><th>PR</th>
              <th>EM</th><th>PR</th><th>EM</th><th>PR</th>
            </tr>
          </thead>
          <tbody>
            <tr class="group-header"><td colspan="17">Proprietary Models</td></tr>
            <tr>
              <td class="model-cell">GPT-5.1</td>
              <td>üìñ+üñºÔ∏è</td>
              <td>üìñ</td>
              <td>10.6</td><td>10.7</td>
              <td>6.32</td><td>6.72</td><td>6.00</td><td>6.00</td>
              <td>0</td><td>0</td><td>0</td><td>0</td>
              <td>0</td><td>0</td><td>0</td><td>0</td>
            </tr>
            <tr>
              <td class="model-cell">GPT-5.2</td>
              <td>üìñ+üñºÔ∏è</td>
              <td>üìñ</td>
              <td>12.5</td><td>12.5</td>
              <td>8.40</td><td>8.40</td><td>8.40</td><td>8.40</td>
              <td>0</td><td>0</td><td>0</td><td>0</td>
              <td>0</td><td>0</td><td>0</td><td>0</td>
            </tr>
            <tr class="divider-row"><td colspan="17"></td></tr>
            <tr class="group-header"><td colspan="17">Open-Sourced Models (All Fine-Tuned)</td></tr>
            <tr>
              <td class="model-cell">Qwen3-VL-8B</td>
              <td>üìñ+üñºÔ∏è</td>
              <td>üìñ</td>
              <td>58.3</td><td>68.6</td>
              <td>20.0</td><td>37.3</td><td>19.2</td><td>34.3</td>
              <td>0</td><td>13.3</td><td>0</td><td>13.2</td>
              <td>0</td><td>11.3</td><td>0</td><td>8.9</td>
            </tr>
            <tr>
              <td class="model-cell">&nbsp;&nbsp;- w coordinates</td>
              <td></td><td></td>
              <td>72.0</td><td>77.3</td>
              <td>33.2</td><td>45.0</td><td>22.0</td><td>30.5</td>
              <td>0</td><td>17.1</td><td>0</td><td>13.4</td>
              <td>0</td><td>8.1</td><td>0</td><td>5.9</td>
            </tr>
            <tr class="overall"><td class="model-cell">VPRL-7B *</td>
              <td>üñºÔ∏è</td><td>üñºÔ∏è</td>
              <td>73.5</td><td>78.6</td>
              <td>14.0</td><td>25.2</td><td>4.00</td><td>6.20</td>
              <td>0</td><td>11.0</td><td>2.00</td><td>16.7</td>
              <td>0</td><td>4.10</td><td>0</td><td>0.70</td>
            </tr>
            <tr class="best"><td class="model-cell">Wan2.2-TI2V-5B</td>
              <td>üìñ+üñºÔ∏è</td><td>üéûÔ∏è</td>
              <td>96.0</td><td>99.0</td>
              <td>90.0</td><td>92.3</td><td>80.0</td><td>83.6</td>
              <td>44.0</td><td>55.2</td><td>42.0</td><td>51.6</td>
              <td>40.0</td><td>51.1</td><td>32.0</td><td>47.1</td>
            </tr>
            <tr class="best"><td class="model-cell">&nbsp;&nbsp;- Unseen Visual Icons</td>
              <td></td><td></td>
              <td>95.5</td><td>98.2</td>
              <td>92.0</td><td>92.6</td><td>78.0</td><td>81.6</td>
              <td>36.0</td><td>46.3</td><td>42.0</td><td>52.0</td>
              <td>38.0</td><td>47.9</td><td>32.0</td><td>42.3</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
  </div>
</section>
  <!-- Tangram Quantitative Results -->
  <section class="section leaderboard-section">
    <div class="container is-max-desktop">
      <div class="leaderboard-card">
        <h2 class="leaderboard-title">Tangram Quantitative Results</h2>
        <p class="leaderboard-subtitle">GC. = Goal Completion, BA. = Boundary Adherence. Models are tested on seen puzzle patterns during training for learnability, and unseen patterns for generalizability.</p>
        <div class="leaderboard-table-wrap">
          <table class="leaderboard-table tangram-table">
            <thead>
              <tr>
                <th rowspan="2">Model</th>
                <th rowspan="2">Input</th>
                <th rowspan="2">Output</th>
                <th colspan="3">Seen (Learnability)</th>
                <th colspan="3">Unseen (Generalizability)</th>
              </tr>
              <tr>
                <th>Strict GC</th>
                <th>Progress GC</th>
                <th>BA</th>
                <th>Strict GC</th>
                <th>Progress GC</th>
                <th>BA</th>
              </tr>
            </thead>
            <tbody>
              <tr class="group-header"><td colspan="9">Fade-In</td></tr>
              <tr class="best">
                <td class="model-cell">Qwen-Image-Edit-20B</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üñºÔ∏è</td>
                <td>31.0</td><td>82.3</td><td>99.8</td>
                <td>32.0</td><td>81.3</td><td>99.7</td>
              </tr>
              <tr class="best">
                <td class="model-cell">Wan2.2-TI2V-5B</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üéûÔ∏è</td>
                <td>0.80</td><td>49.4</td><td>98.1</td>
                <td>0.80</td><td>48.9</td><td>98.0</td>
              </tr>

              <tr class="divider-row"><td colspan="9"></td></tr>
              <tr class="group-header"><td colspan="9">Rotation</td></tr>
              <tr>
                <td class="model-cell">Qwen3-VL-8B</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üìñ</td>
                <td>14.4</td><td>69.7</td><td>89.5</td>
                <td>1.6</td><td>52.1</td><td>80.8</td>
              </tr>
              <tr class="best">
                <td class="model-cell">Nano Banana</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üñºÔ∏è</td>
                <td>-</td><td>-</td><td>-</td>
                <td>9.80</td><td>43.4</td><td>64.7</td>
              </tr>
              <tr class="best">
                <td class="model-cell">Qwen-Image-Edit-20B</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üñºÔ∏è</td>
                <td>45.2</td><td>87.5</td><td>99.7</td>
                <td>43.2</td><td>85.7</td><td>99.6</td>
              </tr>
              <tr class="best">
                <td class="model-cell">Wan2.2-TI2V-5B</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üéûÔ∏è</td>
                <td>22.4</td><td>76.8</td><td>98.1</td>
                <td>22.4</td><td>74.5</td><td>98.0</td>
              </tr>

              <tr class="divider-row"><td colspan="9"></td></tr>
              <tr class="group-header"><td colspan="9">Translation</td></tr>
              <tr>
                <td class="model-cell">Qwen3-VL-8B</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üìñ</td>
                <td>28.0</td><td>75.7</td><td>91.4</td>
                <td>1.60</td><td>58.9</td><td>82.4</td>
              </tr>
              <tr class="best">
                <td class="model-cell">Nano Banana</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üñºÔ∏è</td>
                <td>-</td><td>-</td><td>-</td>
                <td>3.90</td><td>51.3</td><td>74.5</td>
              </tr>
              <tr class="best">
                <td class="model-cell">Qwen-Image-Edit-20B</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üñºÔ∏è</td>
                <td>85.7</td><td>97.7</td><td>99.9</td>
                <td>76.0</td><td>95.4</td><td>99.7</td>
              </tr>
              <tr class="best">
                <td class="model-cell">Wan2.2-TI2V-5B</td>
                <td>üìñ+üñºÔ∏è</td>
                <td>üéûÔ∏è</td>
                <td>68.0</td><td>94.7</td><td>97.0</td>
                <td>60.8</td><td>92.0</td><td>97.0</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </section>

<!--End Result Table -->



<section class="hero is-small is-light">
<div class="hero-body">
  <div class="container">
    <h2 class="title">More Qualitative Examples</h2>

    <!-- TODO: Replace with your poster PDF -->
    <iframe  src="static/pdfs/demonstrations.pdf" width="100%" height="550">
        </iframe>
      
    </div>
  </div>
</section>


  <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{li2026thinkingframes,
  title={Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning},
  author={Chengzu Li and Zanyi Wang and Jiaang Li and Yi Xu and Han Zhou and Huanyu Zhang and Ruichuan An and Dengyang Jiang and Zhaochong An and Ivan Vuliƒá and Serge Belongie and Anna Korhonen},
  journal={arXiv preprint arXiv:2601.21037},
  year={2026}
}</code></pre>
      </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
